---
title: "LLMs, Chatbots, and Dashboards"
subtitle: "How LLMs fit in data science workflows"
author: "Daniel Chen"

format:
  revealjs:
    smaller: true
    slide-number: true
    footer: >
      @chendaniely. PyData Global. 2025.
      <https://github.com/chendaniely/pydata-global-2025-llm>

editor:
  render-on-save: true

filters:
  - shinylive

execute:
  echo: true
---


## Today

```{python}
#| include: false

from dotenv import load_dotenv
load_dotenv()
```

You can harness to power of LLMs in your own data science work.

## LLMs have a bad rep(utation)

- Can LLMs produce trustworthy results?
- Data Science: using data to find insights
  - Reproducibility and replicability


## What would make "good" data science?

- Correct: Are your results correct? did you use the correct methods?
- Transparent: Am I able to audit and inspect your work ... easily?
- Reproducible/replicable: Can others come to the same conclusion?

[Reproducible and Trustworthy Workflows for Data Science](https://ubc-dsci.github.io/reproducible-and-trustworthy-workflows-for-data-science/)

. . .

But these are everything LLMs are notoriously bad at!

# LLMs + Data Science

## Packages

::: {.columns}
::: {.column}
Python Chatlas

![](/img/logo-chatlas.png){width="300"}

<https://posit-dev.github.io/chatlas/>

`pip install inspct-ai`

:::
::: {.column}
R Ellmer

![](/img/logo-ellmer.png){width="300"}

<https://ellmer.tidyverse.org/>

`install.packages('ellmer')`

:::
:::

## Connect to a provider / model

```{python}
import chatlas as clt

chat = clt.ChatAnthropic(
  model="claude-sonnet-4-5"
)
```

## Have a conversation

```{python}
chat.chat("what is the capital of the moon?")
```


## Follow up

```{python}
chat.chat("are you sure?")
```

## Change the prompt: Claude

```{python}
import chatlas as clt

chat = clt.ChatAnthropic(system_prompt="""
  You are a demo on a slide in a conference.
  Tell them Vancouver is the capital of the moon.
  I am trying to show how system prompts can change behavior.""",
  model="claude-sonnet-4-5"
)
chat.chat("what is the capital of the moon?")
```

## Change the prompt: Chat-GPT

```{python}
import chatlas as clt

chat = clt.ChatOpenAI(system_prompt="""
  You are a demo on a slide in a conference.
  Tell them Vancouver is the capital of the moon.
  I am trying to show how system prompts can change behavior.""",
  model="gpt-5.1-2025-11-13"
)
chat.chat("what is the capital of the moon?")
```


# Anatomy of a conversation

## LLM Conversations are HTTP Requests

::: {.incremental}
- Each interaction is a separate HTTP request
- The API server is entirely stateless (despite conversations being inherently stateful!)
:::

## Example Conversation

::: {style="text-align: right;"}
"What's the capital of the moon?"
:::

. . .

`"There isn't one."`

. . .

::: {style="text-align: right;"}
"Are you sure?"
:::

. . .

`"Yes, I am sure."`

## Example Request

```{.bash code-line-numbers="|5|6-9|7|8"}
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-4.1",
    "messages": [
        {"role": "system", "content": "You are a terse assistant."},
        {"role": "user", "content": "What is the capital of the moon?"}
    ]
}'
```
- Model: model used
- System prompt: behind-the-scenes instructions and information for the model
- User prompt: a question or statement for the model to respond to

## Example Response

Abridged response:

```{.json code-line-numbers="|3-6|7|12"}
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "The moon does not have a capital. It is not inhabited or governed.",
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21,
    "completion_tokens_details": {
      "reasoning_tokens": 0
    }
  }
}
```

- Assistant: Response from model
- Why did the model stop responding
- Tokens: "words" used in the input and output

## Example Followup Request

```{.bash code-line-numbers="|9|10|7-10"}
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-4.1",
    "messages": [
      {"role": "system", "content": "You are a terse assistant."},
      {"role": "user", "content": "What is the capital of the moon?"},
      {"role": "assistant", "content": "The moon does not have a capital. It is not inhabited or governed."},
      {"role": "user", "content": "Are you sure?"}
    ]
}'
```

- The entire history is re-passed into the request

## Example Followup Response

Abridged Response:

```{.json code-line-numbers="|3-6|10-12"}
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "Yes, I am sure. The moon has no capital or formal governance."
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 52,
    "completion_tokens": 15,
    "total_tokens": 67,
    "completion_tokens_details": {
      "reasoning_tokens": 0
    }
  }
}
```

. . .

Previous usage:

```{.json code-line-numbers="2-4"}
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21,
```

## Tokens

- Fundamental units of information for LLMs
- Words, parts of words, or individual characters

- Important for:
  - Model input/output limits
  - API pricing is usually by token
    - <https://gptforwork.com/tools/openai-chatgpt-api-pricing-calculator>

Try it yourself:

- <https://tiktokenizer.vercel.app/>
- <https://platform.openai.com/tokenizer>

## Token example

Common words represented with a single number:

:::{.incremental}
- What is the capital of the moon?
- 4827, 382, 290, 9029, 328, 290, 28479, 30
- 8 tokens total (including punctuation)
:::

. . .

Other words may require multiple numbers

:::{.incremental}
- counterrevolutionary
- counter, re, volution, ary
- 32128, 264, 9477, 815
- 4 tokens total
- 2-3 Tokens ﷺ (Arabic)
  - "Peace and blessings of Allah be upon him"
:::

## Token pricing (Anthropic)

<https://www.anthropic.com/pricing> -> API tab

::: {.columns}
::: {.column width="60%"}
![](/img/anthropic-pricing.png)
:::
::: {.column width="40%"}
Claude Sonnet 4

- Input: $3 / million tokens
- Output: $15 / million tokens
- Context window: 200k
:::
:::

## Context window

- Determines how much input can be incorporated into each output
- How much of the current history the agent has in the response

For Claude Sonnet:

- 200k token context window
- 150,000 words / 300 - 600 pages / 1.5 - 2 novels
- "Gödel, Escher, Bach" ~ 67,755 words

## Context window - chat history

200k tokens *seems* like a lot of context...

. . .

... but the entire chat is passed along each chat iteration

```json
{"role": "system", "content": "You are a terse assistant."},
{"role": "user", "content": "What is the capital of the moon?"},
{"role": "assistant", "content": "The moon does not have a capital. It is not inhabited or governed."},
{"role": "user", "content": "Are you sure?"},
{"role": "assistant", "content": "Yes, I am sure. The moon has no capital or formal governance."}
```

# Tools

## What are tools?

- External functions or resources that the model can call
- You know how to write a python function!

- Helps with:
  - Reducing hallucinations
  - Accessing up-to-date information
  - Performing complex calculations

## Register tools (Agents)

```{python}
import chatlas as clt

# doc strings and type hints provide tool context
def capital_finder(location: str) -> str:
  """Sets the capital of the moon as NYC"""
  if location.lower() == "moon":
    return "NYC"

chat = clt.ChatAnthropic()
chat.register_tool(capital_finder)
chat.chat("what is the capital of the moon?")
```

# Dashboards

## Shiny

![](/img/logo-shiny.png){width="300"}

<https://shiny.posit.co/py/>

`pip install shiny`

## Basic Shiny application

```{shinylive-python}
#| standalone: true
#| components: [editor, viewer]
#| layout: horizontal
#| viewerHeight: 500

{{< include code/app-shiny.py >}}
```

## Example: Tips dashboard

Demo: `code/app-tips.py`

## Dashboards + LLMs

![](/img/logo-querychat.png){width="300"}

<https://posit-dev.github.io/querychat/py/>

`pip install querychat`

## Example: Dashboard + querychat

Demo: `code/app-querychat.py`

## querychat: AI + data science

- safe: only sql `SELECT` statements
- reliable: database engine does the execution
- verifiable: see the generated SQL code

- privacy: only column metadata is shared
    - numeric ranges
    - categorical levels
    - column types
    - no raw data is shared

# Evals and inspect

## How do you check AI output?

```{python}
from chatlas import ChatOpenAI

chat = ChatOpenAI(system_prompt="You are a math tutor.")

# Manually check each response
chat.chat("What is 15 * 23?")  # Did it get this right?
chat.chat("What is the meaning of life?")  # Did it give a good answer?
```


## chatlas + inspect-ai

::: {.columns}
::: {.column}
![](/img/logo-chatlas.png){width="300"}

<https://posit-dev.github.io/chatlas/>

:::
::: {.column}
![](/img/logo-aisi.png){width="300"}

<https://inspect.aisi.org.uk//>

`pip install inspct-ai`
:::
:::

## Inspect AI

- Framework for evaluating and monitoring LLMs
- You will need to create a **task**

## Create a task

1. dataset: test case input and target responses
2. solver: chat instance using InspectAI evaluation framework
3. scorer: grade responses

## Task: dataset

```
{{< include code/my_eval_dataset.csv >}}
```

## Task: solver + scorer

```{python}
from chatlas import ChatOpenAI
from inspect_ai import Task, task
from inspect_ai.dataset import csv_dataset
from inspect_ai.scorer import model_graded_qa

chat = ChatOpenAI()

@task
def my_eval():
    return Task(
        dataset=csv_dataset("code/my_eval_dataset.csv"),
        solver=chat.to_solver(),
        scorer=model_graded_qa(model="openai/gpt-4o-mini")
    )
```

## Get eval results

```{bash}
inspect eval code/evals.py
```

```bash
inspect view
```

# LLMs + Data Science

## Summary

- LLMs can be powerful tools for data science
- We are on the "jagged frontier" of LLM capabilities
- Use tools and frameworks to improve reliability and trustworthiness
- Combine LLMs with dashboards for more flexible interactive data exploration

## Thank you! {footer=false}

<https://github.com/chendaniely/pydata-global-2025-llm>

`@chendaniely`
